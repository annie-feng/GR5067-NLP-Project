{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import enchant\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Random Forest & MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150232 entries, 0 to 150231\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   business_id  150232 non-null  object \n",
      " 1   cool         150232 non-null  int64  \n",
      " 2   date         150232 non-null  object \n",
      " 3   funny        150232 non-null  float64\n",
      " 4   review_id    150232 non-null  object \n",
      " 5   stars        150232 non-null  float64\n",
      " 6   text         150232 non-null  object \n",
      " 7   useful       150232 non-null  float64\n",
      " 8   user_id      150232 non-null  object \n",
      "dtypes: float64(3), int64(1), object(5)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train_reviews.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 10,000 samples from the original data\n",
    "selected_data = data.sample(10000, random_state = 0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high', 'low', 'low', 'high', 'high', 'low', 'high', 'high', 'high', 'high']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a binary response for each review, review with stars > 3 is assigned as \"high\", otherwise is assigned as\"low\"\n",
    "binary = []\n",
    "for i in selected_data.stars:\n",
    "    if i >= 3:\n",
    "        binary.append(\"high\")\n",
    "    elif i < 3:\n",
    "        binary.append(\"low\")\n",
    "binary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data['binary'] = binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97248</td>\n",
       "      <td>WunR7VclAddvbCnc-97jzg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-07-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>eedlPRo631GSaNX_DaphkQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yes, believe it or not there is really good So...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tgFn7JadPQJ8aYdwIRi8aA</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127659</td>\n",
       "      <td>6vNMmkttsHkW1THWiP50xg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-06-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EDD8yhtd3LQKO95CyOl_MQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>First tip the whole groupon thing is a sham as...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rLpq9zNFLgyVYQrN8Gn-Zg</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16213</td>\n",
       "      <td>EWmwbOm_4UhOtvLaBzHpPA</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-08-27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>FDDFJlUc6kIM_ZTg_23Xsw</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The Skinny: overcooked pasta, wilted iceberg a...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>yXYuW-2Q0X7f0a9MHyERgw</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121599</td>\n",
       "      <td>db12Hn9hdoE-Ne4_NsVKSw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FEe6cMH54x7Vgn_NXl3mEA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I really wanted to love this place. I really d...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>wjejB1QIPsnFOVUmNjaHaA</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119819</td>\n",
       "      <td>IpLvJEjb4AN8eWq5NE2TVA</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ZdvF50_PGujN1g1Juur_lw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>We flew on Delta in and out of PSHIA. As an Ai...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yiHXyCxPenNg5ZWCXF95qQ</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index             business_id  cool        date  funny  \\\n",
       "0   97248  WunR7VclAddvbCnc-97jzg     0  2014-07-23    0.0   \n",
       "1  127659  6vNMmkttsHkW1THWiP50xg     0  2015-06-17    1.0   \n",
       "2   16213  EWmwbOm_4UhOtvLaBzHpPA     3  2010-08-27    4.0   \n",
       "3  121599  db12Hn9hdoE-Ne4_NsVKSw     0  2015-04-20    0.0   \n",
       "4  119819  IpLvJEjb4AN8eWq5NE2TVA     0  2015-04-02    0.0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eedlPRo631GSaNX_DaphkQ    4.0   \n",
       "1  EDD8yhtd3LQKO95CyOl_MQ    2.0   \n",
       "2  FDDFJlUc6kIM_ZTg_23Xsw    2.0   \n",
       "3  FEe6cMH54x7Vgn_NXl3mEA    3.0   \n",
       "4  ZdvF50_PGujN1g1Juur_lw    4.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Yes, believe it or not there is really good So...     0.0   \n",
       "1  First tip the whole groupon thing is a sham as...     1.0   \n",
       "2  The Skinny: overcooked pasta, wilted iceberg a...     3.0   \n",
       "3  I really wanted to love this place. I really d...     5.0   \n",
       "4  We flew on Delta in and out of PSHIA. As an Ai...     1.0   \n",
       "\n",
       "                  user_id binary  \n",
       "0  tgFn7JadPQJ8aYdwIRi8aA   high  \n",
       "1  rLpq9zNFLgyVYQrN8Gn-Zg    low  \n",
       "2  yXYuW-2Q0X7f0a9MHyERgw    low  \n",
       "3  wjejB1QIPsnFOVUmNjaHaA   high  \n",
       "4  yiHXyCxPenNg5ZWCXF95qQ   high  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   index        10000 non-null  int64  \n",
      " 1   business_id  10000 non-null  object \n",
      " 2   cool         10000 non-null  int64  \n",
      " 3   date         10000 non-null  object \n",
      " 4   funny        10000 non-null  float64\n",
      " 5   review_id    10000 non-null  object \n",
      " 6   stars        10000 non-null  float64\n",
      " 7   text         10000 non-null  object \n",
      " 8   useful       10000 non-null  float64\n",
      " 9   user_id      10000 non-null  object \n",
      " 10  binary       10000 non-null  object \n",
      "dtypes: float64(3), int64(2), object(6)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "selected_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high    8485\n",
       "low     1515\n",
       "Name: binary, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data.binary.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, believe it or not there is really good Soul Food in Ahwatukee.  Love the breaded catfish, the fried chicken, and especially the greens.  Lots of flavor to them, and they're not drenched in butter.  The mac and cheese is good.  The cornbread is a bit of a disappointment, as is the sauteed cabbage, but by and large the food is very good, and the owners really friendly and proud of what they're doing.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text, i.e. remove punctuation, change all characters in lower case\n",
    "def clean_text(str_in):\n",
    "    import re\n",
    "    tmp = re.sub(\"[^A-z ]+\", \"\", str_in.lower())\n",
    "    return tmp\n",
    "\n",
    "selected_data['clean_review'] = selected_data.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes believe it or not there is really good soul food in ahwatukee  love the breaded catfish the fried chicken and especially the greens  lots of flavor to them and theyre not drenched in butter  the mac and cheese is good  the cornbread is a bit of a disappointment as is the sauteed cabbage but by and large the food is very good and the owners really friendly and proud of what theyre doing'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data.clean_review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if words are in english dictionary\n",
    "def check_en(var):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    tmp = var.split() #tokenize\n",
    "    tmp_list = list()\n",
    "    for word in tmp:\n",
    "        if d.check(word):\n",
    "            tmp_list.append(word)\n",
    "    tmp = ' '.join(tmp_list)\n",
    "    return tmp\n",
    "\n",
    "selected_data['clean_review'] = selected_data.clean_review.apply(check_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes believe it or not there is really good soul food in love the breaded catfish the fried chicken and especially the greens lots of flavor to them and not drenched in butter the mac and cheese is good the cornbread is a bit of a disappointment as is the sauteed cabbage but by and large the food is very good and the owners really friendly and proud of what doing'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data.clean_review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(var):\n",
    "    my_sw = (stopwords.words('english'))\n",
    "    tmp = var.split()\n",
    "    tmp = [word for word in tmp if my_sw.count(word) == 0]\n",
    "    tmp = ' '.join(tmp)\n",
    "    return tmp\n",
    "\n",
    "selected_data['clean_review'] = selected_data.clean_review.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes believe really good soul food love breaded catfish fried chicken especially greens lots flavor drenched butter mac cheese good cornbread bit disappointment sauteed cabbage large food good owners really friendly proud'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data.clean_review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "def my_stem(var):\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stem = PorterStemmer()\n",
    "    tmp = var.split()\n",
    "    tmp = [stem.stem(word) for word in tmp]\n",
    "    tmp = ' '.join(tmp)\n",
    "    return tmp\n",
    "\n",
    "selected_data['clean_review'] = selected_data.clean_review.apply(my_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ye believ realli good soul food love bread catfish fri chicken especi green lot flavor drench butter mac chees good cornbread bit disappoint saute cabbag larg food good owner realli friendli proud'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data.clean_review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vectorize text\n",
    "def my_tfidf(df_in):\n",
    "    tf_idf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vec_out = pd.DataFrame(tf_idf_vectorizer.fit_transform(df_in).toarray())\n",
    "    tfidf_vec_out.columns = tf_idf_vectorizer.get_feature_names()\n",
    "    return tfidf_vec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aah</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abacu</th>\n",
       "      <th>abalon</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdomin</th>\n",
       "      <th>abhor</th>\n",
       "      <th>...</th>\n",
       "      <th>zinger</th>\n",
       "      <th>zingi</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippi</th>\n",
       "      <th>zither</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zucchini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 11109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aah   ab  aback  abacu  abalon  abandon  abbrevi  abdomen  abdomin  \\\n",
       "0     0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "1     0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "2     0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "3     0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "4     0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "...   ...  ...    ...    ...     ...      ...      ...      ...      ...   \n",
       "9995  0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "9996  0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "9997  0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "9998  0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "9999  0.0  0.0    0.0    0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      abhor  ...  zinger  zingi  zip  zippi  zither  zombi  zone  zoo  zoom  \\\n",
       "0       0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "1       0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "2       0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "3       0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "4       0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "...     ...  ...     ...    ...  ...    ...     ...    ...   ...  ...   ...   \n",
       "9995    0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "9996    0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "9997    0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "9998    0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "9999    0.0  ...     0.0    0.0  0.0    0.0     0.0    0.0   0.0  0.0   0.0   \n",
       "\n",
       "      zucchini  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "...        ...  \n",
       "9995       0.0  \n",
       "9996       0.0  \n",
       "9997       0.0  \n",
       "9998       0.0  \n",
       "9999       0.0  \n",
       "\n",
       "[10000 rows x 11109 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_text = my_tfidf(selected_data.clean_review)\n",
    "tfidf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save object\n",
    "def save_df(df_in, path_in, name_in):\n",
    "    import pickle\n",
    "    try:\n",
    "        pickle.dump(df_in, open(path_in + name_in + \".pkl\", \"wb\"))\n",
    "    except:\n",
    "        print (\"SAVE FAILED\")\n",
    "        pass\n",
    "\n",
    "# open object\n",
    "def open_df(path_in, name_in):\n",
    "    import pickle\n",
    "    try:\n",
    "        df_out = pickle.load(open(path_in + name_in + \".pkl\", \"rb\"))\n",
    "    except:\n",
    "        print (\"OPEN FAILED\")\n",
    "        pass\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE FAILED\n"
     ]
    }
   ],
   "source": [
    "# save tfidf_text\n",
    "save_df(tfidf_text, 'saved_object/', 'tfidf_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPEN FAILED\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df_out' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-948ed2411b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load saved tfidf_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_object/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tfidf_text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtfidf_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-8c094765e3be>\u001b[0m in \u001b[0;36mopen_df\u001b[0;34m(path_in, name_in)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"OPEN FAILED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'df_out' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# load saved tfidf_text\n",
    "tfidf_text = open_df('saved_object/', 'tfidf_text')\n",
    "tfidf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_fun(df_in):\n",
    "    my_pca = PCA(n_components=0.95)\n",
    "    dim_reduced_data = my_pca.fit_transform(df_in)\n",
    "    print (\"explained variance: \" + str(np.sum(my_pca.explained_variance_ratio_)))\n",
    "    \n",
    "    return dim_reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance: 0.950024768821888\n"
     ]
    }
   ],
   "source": [
    "pca_data = pca_fun(tfidf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE FAILED\n"
     ]
    }
   ],
   "source": [
    "# save pca_data\n",
    "save_df(pca_data, 'saved_object/', 'pca_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPEN FAILED\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df_out' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-775e061f1661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load saved pca_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpca_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_object/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pca_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpca_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-8c094765e3be>\u001b[0m in \u001b[0;36mopen_df\u001b[0;34m(path_in, name_in)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"OPEN FAILED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'df_out' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# load saved pca_data\n",
    "pca_data = open_df('saved_object/', 'pca_data')\n",
    "pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models - Predict Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_data, selected_data.stars, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "my_model = RandomForestClassifier(random_state=0)\n",
    "\n",
    "parameters = {'n_estimators':[10, 100], 'max_depth': [25, 50]}\n",
    "\n",
    "my_grid = GridSearchCV(my_model, parameters, cv=5)\n",
    "my_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters\n",
    "my_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model using best parameters and fit training data\n",
    "my_model_optimal = RandomForestClassifier(**my_grid.best_params_, random_state=0)\n",
    "my_model_optimal.fit(X_train, y_train)   \n",
    "y_pred = my_model_optimal.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Test accuracy\n",
    "print(\"Training Accuracy is\", my_model_optimal.score(X_train, y_train)*100, \"%\")\n",
    "print(\"Test Accuracy is\", my_model_optimal.score(X_test, y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "metrics = pd.DataFrame(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "metrics.index = [\"precision\", \"recall\", \"fscore\", \"support\"]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying different hidden_layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build MLP classifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes = (200, 100, 5))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Test accuracy\n",
    "print(\"Training Accuracy is\", clf.score(X_train, y_train)*100, \"%\")\n",
    "print(\"Test Accuracy is\", clf.score(X_test, y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build MLP classifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes = (200, 5))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training & Test accuracy\n",
    "print(\"Training Accuracy is\", clf.score(X_train, y_train)*100, \"%\")\n",
    "print(\"Test Accuracy is\", clf.score(X_test, y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics = pd.DataFrame(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "metrics.index = [\"precision\", \"recall\", \"fscore\", \"support\"]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build MLP classifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes = (100, 5))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Test accuracy\n",
    "print(\"Training Accuracy is\", clf.score(X_train, y_train)*100, \"%\")\n",
    "print(\"Test Accuracy is\", clf.score(X_test, y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models - Predict High/Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_data, selected_data.binary, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "my_model = RandomForestClassifier(random_state=0)\n",
    "\n",
    "parameters = {'n_estimators':[10, 100], 'max_depth': [25, 50]}\n",
    "\n",
    "my_grid = GridSearchCV(my_model, parameters, cv=5)\n",
    "my_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters\n",
    "my_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model using best parameters and fit training data\n",
    "my_model_optimal = RandomForestClassifier(**my_grid.best_params_, random_state=0)\n",
    "\n",
    "my_model_optimal.fit(X_train, y_train)   \n",
    "y_pred = my_model_optimal.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Test accuracy\n",
    "print(\"Training Accuracy is\", my_model_optimal.score(X_train, y_train)*100, \"%\")\n",
    "print(\"Test Accuracy is\", my_model_optimal.score(X_test, y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "metrics = pd.DataFrame(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "metrics.index = [\"precision\", \"recall\", \"fscore\", \"support\"]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying different hidden_layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build MLP classifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes = (10, 5))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Test accuracy\n",
    "print(\"Training Accuracy is\", clf.score(X_train, y_train)*100, \"%\")\n",
    "print(\"Test Accuracy is\", clf.score(X_test, y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics = pd.DataFrame(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "metrics.index = [\"precision\", \"recall\", \"fscore\", \"support\"]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build MLP classifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes = (5, 5))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Test accuracy\n",
    "print(\"Training Accuracy is\", clf.score(X_train, y_train)*100, \"%\")\n",
    "print(\"Test Accuracy is\", clf.score(X_test, y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics = pd.DataFrame(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "metrics.index = [\"precision\", \"recall\", \"fscore\", \"support\"]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build MLP classifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes = (20, 5))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Test accuracy\n",
    "print(\"Training Accuracy is\", clf.score(X_train, y_train)*100, \"%\")\n",
    "print(\"Test Accuracy is\", clf.score(X_test, y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics = pd.DataFrame(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "metrics.index = [\"precision\", \"recall\", \"fscore\", \"support\"]\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
